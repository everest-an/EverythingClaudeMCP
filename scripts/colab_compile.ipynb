{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AwesomeContext Qwen3-4B GPU Compilation\n\nThis notebook compiles all 122 modules using **Qwen3-4B** on a **T4 GPU** (free Colab tier).\n\n> **Runtime Setup**: Go to `Runtime → Change runtime type → T4 GPU`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"No GPU detected! Change runtime to T4 GPU.\"\n",
    "print(f\"\\n✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"✅ VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
    "print(f\"✅ PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!git clone --recurse-submodules https://github.com/everest-an/AwesomeContext.git\n%cd AwesomeContext\n!pip install -e '.[dev]' -q\nprint(\"\\n✅ Dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dry Run — Preview Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.compiler.cli \\\n",
    "    --model-name Qwen/Qwen3-4B \\\n",
    "    --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Full Compilation with Qwen3-4B\n",
    "\n",
    "Compiles all 122 modules. Estimated time: **~2 minutes** on T4 GPU (~1s/module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.compiler.cli \\\n",
    "    --model-name Qwen/Qwen3-4B \\\n",
    "    --output data/tensors-qwen3-4b \\\n",
    "    --index-dir data/index-qwen3-4b \\\n",
    "    --cache-dir data/cache-qwen3-4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/verify_tensors.py --tensor-dir data/tensors-qwen3-4b --index-dir data/index-qwen3-4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "embeddings = np.load('data/index-qwen3-4b/embeddings.npy')\n",
    "with open('data/index-qwen3-4b/manifest.json', encoding='utf-8') as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "entries = manifest['entries']\n",
    "print(f'Modules: {len(entries)}')\n",
    "print(f'Embedding dim: {embeddings.shape[1]}')\n",
    "print(f'Expected dim: 2560 (Qwen3-4B hidden_size)')\n",
    "print()\n",
    "\n",
    "# Type distribution\n",
    "types = Counter(e['module_type'] for e in entries)\n",
    "for t, c in sorted(types.items()):\n",
    "    print(f'  {t}: {c}')\n",
    "\n",
    "# L2 norms\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(f'\\nL2 norms: min={norms.min():.6f}, max={norms.max():.6f}, all~1.0={np.allclose(norms, 1.0, atol=0.01)}')\n",
    "\n",
    "# Semantic queries\n",
    "def find_idx(mid):\n",
    "    for i, e in enumerate(entries):\n",
    "        if e['module_id'] == mid:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "print('\\n--- Security nearest neighbors ---')\n",
    "sec = find_idx('rules/common--security')\n",
    "if sec is not None:\n",
    "    sims = embeddings @ embeddings[sec]\n",
    "    for idx in np.argsort(sims)[::-1][:6]:\n",
    "        if idx != sec:\n",
    "            print(f'  {entries[idx][\"module_id\"]:45s} {sims[idx]:.4f}')\n",
    "\n",
    "print('\\n--- Testing nearest neighbors ---')\n",
    "tst = find_idx('rules/common--testing')\n",
    "if tst is not None:\n",
    "    sims = embeddings @ embeddings[tst]\n",
    "    for idx in np.argsort(sims)[::-1][:6]:\n",
    "        if idx != tst:\n",
    "            print(f'  {entries[idx][\"module_id\"]:45s} {sims[idx]:.4f}')\n",
    "\n",
    "# Global discrimination\n",
    "all_sims = embeddings @ embeddings.T\n",
    "mask = ~np.eye(len(entries), dtype=bool)\n",
    "print(f'\\nGlobal avg sim: {all_sims[mask].mean():.4f}')\n",
    "print(f'Min sim: {all_sims[mask].min():.4f}')\n",
    "print(f'Max sim: {all_sims[mask].max():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Decode Quality\n",
    "\n",
    "This is where Qwen3-4B should shine vs 1.5B — latent-to-text decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.adapter.model_wrapper import AdaptedModelWrapper\n",
    "from src.compiler.persistence import load_module_tensor\n",
    "\n",
    "wrapper = AdaptedModelWrapper(model_name=\"Qwen/Qwen3-4B\")\n",
    "\n",
    "test_modules = [\n",
    "    'rules/common--security',\n",
    "    'skills/python-testing',\n",
    "    'agents/architect',\n",
    "]\n",
    "\n",
    "for mid in test_modules:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Decoding: {mid}')\n",
    "    print('='*60)\n",
    "    embedding = load_module_tensor('data/tensors-qwen3-4b', mid, 'mean_embedding')\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding = embedding.to(wrapper.device)\n",
    "    text = wrapper.decode_from_latent(embedding, intent='Summarize this module')\n",
    "    print(text[:500])\n",
    "\n",
    "wrapper.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Package & Download\n",
    "\n",
    "Download compiled tensors to use on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import shutil\n\n# Package tensors + index into a zip\nshutil.make_archive(\n    'awesome-context-qwen3-4b',\n    'zip',\n    '.',\n    'data/tensors-qwen3-4b'\n)\n\n# Also include the index\n!cd data && zip -r ../awesome-context-qwen3-4b.zip index-qwen3-4b/ cache-qwen3-4b/\n\n# Download\nfrom google.colab import files\nfiles.download('awesome-context-qwen3-4b.zip')\nprint(f'\\n✅ Download started. Unzip into your local data/ directory.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Local Usage After Download\n\n```bash\n# On your local machine:\ncd AwesomeContext\nunzip awesome-context-qwen3-4b.zip -d data/\n\n# Point the server to the Qwen3-4B tensors:\nexport AC_TENSOR_DIR=data/tensors-qwen3-4b\nexport AC_INDEX_DIR=data/index-qwen3-4b\npython scripts/serve.py\n```"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}